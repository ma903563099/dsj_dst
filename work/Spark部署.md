#  Spark 组件部署

1、需前置 Hadoop 环境，并检查 Hadoop 环境是否可用，截图并保存结果；  
2、解压 scala 安装包到“/usr/local/”路径下，并更名为 scala，截图并保存
结果；  
3、设置 scala 环境变量，并使环境变量只对当前用户生效，截图并保存结果；  
4、进入 scala 并截图，截图并保存结果；  
5、解压 Spark 安装包到“/usr/local/”路径下，并更名为 spark，截图并保
存结果；  
6、设置 Spark 环境变量，并使环境变量只对当前用户生效，截图并保存结果；  
7、修改 Spark 参数配置，指定 Spark slave 节点，截图并保存结果；  
8、启动 Spark，并使用命令查看 webUI 结果，截图并保存结果；  
  
> 显示图如以下示例：  
## 主节点
![](http://tmp.mada8.com/201905221618_876.png)

## 从节点
![](http://tmp.mada8.com/201905221619_125.png)